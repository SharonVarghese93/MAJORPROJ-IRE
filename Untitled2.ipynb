{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Dense, Input, Embedding, Concatenate,InputSpec\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from scipy import stats\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "from keras.layers import Dense, LSTM, Input, Embedding, GlobalAveragePooling1D, Concatenate, Activation, Lambda, BatchNormalization, Convolution1D, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import itertools\n",
    "import re\n",
    "import os\n",
    "from collections import Counter\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_Tensor_layer(Layer):\n",
    "    def __init__(self,output_dim,input_dim=None, **kwargs):\n",
    "        self.output_dim=output_dim\n",
    "        self.input_dim=input_dim\n",
    "        if self.input_dim:\n",
    "            kwargs['input_shape']=(self.input_dim,)\n",
    "        super(Neural_Tensor_layer,self).__init__(**kwargs)\n",
    "\n",
    "    def build(self,input_shape):\n",
    "        mean=0.0\n",
    "        std=1.0\n",
    "        k=self.output_dim\n",
    "        d=self.input_dim\n",
    "        W_val=stats.truncnorm.rvs(-2 * std, 2 * std, loc=mean, scale=std, size=(k,d,d))\n",
    "        V_val=stats.truncnorm.rvs(-2 * std, 2 * std, loc=mean, scale=std, size=(2*d,k))\n",
    "        self.W=K.variable(W_val)\n",
    "        self.V=K.variable(V_val)\n",
    "        self.b=K.zeros((self.input_dim,))\n",
    "        self.trainable_weights=[self.W,self.V,self.b]\n",
    "\n",
    "    def call(self,inputs,mask=None):\n",
    "        e1=inputs[0]\n",
    "        e2=inputs[1]\n",
    "        batch_size=K.shape(e1)[0]\n",
    "        k=self.output_dim\n",
    "\n",
    "\n",
    "        feed_forward=K.dot(K.concatenate([e1,e2]),self.V)\n",
    "\n",
    "        bilinear_tensor_products = [ K.sum((e2 * K.dot(e1, self.W[0])) + self.b, axis=1) ]\n",
    "\n",
    "        for i in range(k)[1:]:\t\n",
    "            btp=K.sum((e2*K.dot(e1,self.W[i]))+self.b,axis=1)\n",
    "            bilinear_tensor_products.append(btp)\n",
    "\n",
    "        result=K.tanh(K.reshape(K.concatenate(bilinear_tensor_products,axis=0),(batch_size,k))+feed_forward)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        batch_size=input_shape[0][0]\n",
    "        return(batch_size,self.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Temporal_Mean_Pooling(Layer): \n",
    "    def __init__(self, **kwargs):\n",
    "        self.input_spec=InputSpec(ndim=3)\n",
    "        super(Temporal_Mean_Pooling,self).__init__(**kwargs)\n",
    "\n",
    "    def call(self,x):\n",
    "        mask=K.mean(K.ones_like(x),axis=-1)\n",
    "        return K.sum(x,axis=-2)/K.sum(mask,axis=-1,keepdims=True)\n",
    "\n",
    "    def compute_output_shape(self,input_shape):\n",
    "        return (input_shape[0],input_shape[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_dim = 300,lstm_op_dim=50,vocab_size=4000,embed_dim=300,max_len=1000,coherence_width=50,k=10, start=3):\n",
    "#     inputs = Input(shape=(max_len,))\n",
    "#     embedding = Embedding(vocab_size, 300, input_length=max_len)(inputs)\n",
    "#     lstm = LSTM(lstm_op_dim, return_sequences = True)(embedding)\n",
    "    inputs = Input(shape=(max_len,input_dim))\n",
    "    lstm = LSTM(lstm_op_dim, return_sequences = True)(inputs)    \n",
    "    bilinear_products=Neural_Tensor_layer(output_dim=k,input_dim=lstm_op_dim)\n",
    "    pairs = [((start + i * coherence_width) % max_len, (start + i * coherence_width + coherence_width) % max_len) for i in range(int(max_len/coherence_width))]\n",
    "    similarity_pairs = [ (Lambda(lambda t: t[:, p[0], :])(lstm), Lambda(lambda t: t[:, p[1], :])(lstm)) for p in pairs]\n",
    "    sigmoid_layer = Dense(1, activation=\"sigmoid\")\n",
    "    similarities = [sigmoid_layer(bilinear_products([w[0], w[1]])) for w in similarity_pairs]  \n",
    "    tmp = Temporal_Mean_Pooling()(lstm)\n",
    "    simi = Concatenate()([i for i in similarities])\n",
    "    tmp_simi = Concatenate()([tmp,simi])\n",
    "    dense1 = Dense(256,activation='relu')(tmp_simi)\n",
    "    dense2 = Dense(64,activation='relu')(dense1)\n",
    "    out = Dense(1,activation='linear')(dense2)\n",
    "    model = Model(inputs = inputs, outputs = out)\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_dim = 300,lstm_op_dim=50,vocab_size=4000,embed_dim=300,max_len=1000,coherence_width=50,k=10, start=3):\n",
    "#     inputs = Input(shape=(max_len,))\n",
    "#     embedding = Embedding(vocab_size, 300, mask_zero=True, input_length=max_len)(inputs)\n",
    "    inputs = Input(shape=(max_len,input_dim))\n",
    "    lstm = LSTM(lstm_op_dim, return_sequences = False)(inputs)\n",
    "    op = Dense(1,activation='linear')(lstm)\n",
    "    model = Model(inputs = inputs, outputs = op)    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mse',lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 650, 300)          0         \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 50)                70200     \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 70,251\n",
      "Trainable params: 70,251\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_data(train_path,essay_set = 1):\n",
    "    train_path = train_path\n",
    "    training_data = pandas.read_excel(train_path, delimiter='\\t')\n",
    "    resolved_score = training_data[training_data['essay_set'] == essay_set]['domain1_score']\n",
    "    essay_ids = training_data[training_data['essay_set'] == essay_set]['essay_id']\n",
    "    essays = training_data[training_data['essay_set'] == essay_set]['essay']\n",
    "    essay_list = []\n",
    "    for idx, essay in essays.iteritems():\n",
    "        essay_list.append(clean_tokenize(essay))\n",
    "    return essay_list, resolved_score.tolist(), essay_ids.tolist()\n",
    "\n",
    "def clean_tokenize(data):\n",
    "    data = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", data)\n",
    "    data = re.sub(r\"\\'s\", \" \\'s\", data)\n",
    "    data = re.sub(r\"\\'ve\", \" \\'ve\", data)\n",
    "    data = re.sub(r\"n\\'t\", \" n\\'t\", data)\n",
    "    data = re.sub(r\"\\'re\", \" \\'re\", data)\n",
    "    data = re.sub(r\"\\'d\", \" \\'d\", data)\n",
    "    data = re.sub(r\"\\'ll\", \" \\'ll\", data)\n",
    "    data = re.sub(r\",\", \" , \", data)\n",
    "    data = re.sub(r\"!\", \" ! \", data)\n",
    "    data = re.sub(r\"\\(\", \" ( \", data)\n",
    "    data = re.sub(r\"\\)\", \" ) \", data)\n",
    "    data = re.sub(r\"\\?\", \" ? \", data)\n",
    "    data = re.sub(r\"\\s{2,}\", \" \", data)\n",
    "    data = data.lower()\n",
    "    return [x.strip() for x in re.split('(\\W+)?', data) if x.strip()]\n",
    "\n",
    "\n",
    "def score_range(self):\n",
    "    return {\"1\": (2, 12),\"2\": (1, 6),\"3\": (0, 3),\"4\": (0, 3),\"5\": (0, 4),\"6\": (0, 4),\"7\": (0, 30),\"8\": (0, 60)}\n",
    "\n",
    "def normalize_score(self, essay_set_id, score):\n",
    "    lo, hi = self.score_range[str(essay_set_id)]\n",
    "    score = float(score)\n",
    "    return (score - lo) / (hi - lo)\n",
    "\n",
    "\n",
    "def vectorize_data(data, word_idx, sentence_size):\n",
    "    E = []\n",
    "    for essay in data:\n",
    "        ls = max(0, sentence_size - len(essay))\n",
    "        wl = []\n",
    "        for w in essay:\n",
    "            if w in word_idx:\n",
    "                wl.append(word_idx[w])\n",
    "            else:\n",
    "                wl.append(0)\n",
    "        wl += [0]*ls\n",
    "        E.append(wl)\n",
    "    return E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertword2vec(dim=300):\n",
    "    word2vec = []\n",
    "    word_idx = {}\n",
    "    word2vec.append([0]*dim)\n",
    "    count = 1\n",
    "    with open('glove.6B.300d.txt') as f:\n",
    "        for line in f:\n",
    "            l = line.split()\n",
    "            word = l[0]\n",
    "            vector = list(map(float, l[1:]))\n",
    "            word_idx[word] = count\n",
    "            word2vec.append(vector)\n",
    "            count += 1\n",
    "    return word_idx, word2vec\n",
    "word_idx, word2vec = convertword2vec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/re.py:203: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "essay_set_id = 1\n",
    "essay_list, resolved_scores, essay_id = load_training_data('training_set_rel3.xlsx',essay_set_id)\n",
    "vocab_size = len(word_idx) + 1\n",
    "max_sent_size = 1000\n",
    "vectorized_data = vectorize_data(essay_list, word_idx, max_sent_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_vec(vectorized_data, word2vec):\n",
    "    total_vector = []\n",
    "    for essay in vectorized_data:\n",
    "        essay_vector = []\n",
    "        for word in essay:\n",
    "            essay_vector.append((word2vec[word]))\n",
    "        total_vector.append(essay_vector)\n",
    "    return total_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = convert_to_vec(vectorized_data, word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1783, 1000, 300)\n"
     ]
    }
   ],
   "source": [
    "X = np.array(X)\n",
    "print (np.shape(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "('Some keys in session_kwargs are not supported at this time: %s', dict_keys(['lr']))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-61fce8145330>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model.compile(optimizer='adam',\n\u001b[1;32m      2\u001b[0m                loss='mse',lr = 0.001)\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mresolved_scores\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1006\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1008\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1009\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_make_train_function\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    506\u001b[0m                     \u001b[0mupdates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m                     \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'train_function'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m                     **self._function_kwargs)\n\u001b[0m\u001b[1;32m    509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_test_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(inputs, outputs, updates, **kwargs)\u001b[0m\n\u001b[1;32m   2693\u001b[0m                 \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Invalid argument \"%s\" passed to K.function with TensorFlow backend'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2694\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2695\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, inputs, outputs, updates, name, **session_kwargs)\u001b[0m\n\u001b[1;32m   2540\u001b[0m             raise ValueError('Some keys in session_kwargs are not '\n\u001b[1;32m   2541\u001b[0m                              \u001b[0;34m'supported at this '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2542\u001b[0;31m                              'time: %s', session_kwargs.keys())\n\u001b[0m\u001b[1;32m   2543\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2544\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_arrays\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: ('Some keys in session_kwargs are not supported at this time: %s', dict_keys(['lr']))"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "               loss='mse',lr = 0.001)\n",
    "model.fit(X,resolved_scores,batch_size = 64,epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val=[int(np.round(a)) for a in resolved_scores]\n",
    "y_pred=[int(np.round(a)) for a in predictions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24, 27, 21, 30, 24, 24, 30, 30, 27, 27, 24, 24, 21, 18, 18, 36, 24, 24, 12, 18, 24, 9, 30, 33, 24, 27, 12, 27, 27, 24, 30, 30, 18, 24, 27, 30, 36, 24, 30, 21, 6, 24, 18, 24, 24, 24, 24, 33, 18, 15, 27, 21, 24, 30, 24, 30, 27, 21, 24, 12, 24, 24, 24, 21, 27, 27, 24, 27, 21, 36, 30, 30, 24, 21, 24, 24, 30, 30, 30, 24, 24, 24, 21, 18, 30, 24, 30, 27, 18, 21, 24, 33, 33, 24, 30, 21, 24, 33, 24, 21, 30, 24, 27, 30, 27, 33, 30, 24, 24, 18, 33, 27, 24, 24, 27, 12, 24, 36, 30, 24, 24, 27, 30, 21, 15, 24, 27, 27, 30, 30, 30, 33, 30, 30, 21, 27, 21, 30, 21, 27, 30, 30, 21, 27, 30, 18, 36, 27, 24, 24, 24, 18, 27, 36, 30, 24, 27, 27, 30, 24, 30, 36, 24, 24, 27, 30, 24, 27, 18, 24, 24, 24, 24, 24, 24, 30, 24, 24, 24, 30, 30, 24, 24, 24, 24, 24, 15, 27, 18, 30, 24, 33, 24, 30, 15, 24, 30, 30, 33, 24, 27, 24, 30, 30, 30, 30, 24, 18, 18, 27, 18, 21, 24, 24, 30, 24, 30, 24, 27, 30, 18, 27, 24, 24, 36, 24, 27, 24, 24, 27, 24, 36, 21, 24, 21, 18, 24, 27, 27, 24, 27, 27, 33, 21, 21, 33, 24, 27, 27, 30, 27, 27, 18, 30, 24, 24, 24, 33, 27, 21, 27, 24, 30, 24, 33, 33, 30, 24, 27, 24, 24, 30, 24, 27, 33, 24, 27, 24, 30, 30, 21, 24, 18, 24, 33, 12, 36, 24, 30, 33, 33, 33, 24, 30, 30, 30, 27, 24, 24, 24, 27, 30, 30, 27, 30, 24, 27, 18, 27, 30, 27, 21, 27, 30, 27, 30, 24, 27, 24, 36, 24, 30, 24, 27, 30, 18, 24, 24, 33, 30, 24, 27, 30, 27, 33, 30, 24, 33, 27, 24, 27, 27, 30, 36, 21, 24, 24, 24, 27, 21, 15, 24, 18, 24, 24, 21, 6, 18, 30, 30, 24, 33, 27, 24, 24, 27, 24, 30, 24, 30, 24, 21, 12, 24, 30, 33, 27, 30, 30, 21, 33, 24, 33, 30, 24, 21, 24, 30, 36, 24, 24, 21, 18, 21, 30, 30, 24, 24, 18, 24, 24, 24, 27, 24, 24, 24, 24, 24, 36, 30, 27, 24, 21, 18, 24, 36, 18, 24, 18, 21, 27, 27, 30, 27, 33, 24, 27, 24, 33, 24, 33, 21, 33, 24, 24, 24, 30, 24, 24, 30, 24, 24, 24, 30, 33, 27, 6, 21, 24, 21, 27, 24, 12, 21, 30, 27, 24, 24, 24, 24, 24, 21, 24, 27, 24, 24, 27, 24, 24, 36, 21, 18, 27, 24, 18, 24, 30, 24, 30, 27, 27, 21, 30, 27, 24, 24, 27, 24, 24, 21, 24, 27, 24, 27, 27, 36, 27, 30, 24, 21, 24, 30, 30, 21, 30, 27, 33, 21, 24, 24, 24, 24, 24, 24, 24, 24, 36, 18, 21, 27, 30, 24, 27, 21, 24, 18, 21, 30, 30, 24, 24, 33, 24, 24, 24, 24, 27, 24, 24, 30, 24, 24, 6, 18, 24, 24, 24, 30, 24, 18, 24, 24, 33, 30, 30, 30, 36, 24, 24, 30, 21, 24, 27, 24, 24, 24, 24, 24, 27, 33, 30, 24, 30, 24, 18, 27, 24, 24, 24, 33, 27, 27, 33, 24, 27, 30, 15, 18, 24, 24, 30, 21, 24, 18, 24, 24, 24, 18, 24, 24, 33, 27, 18, 24, 27, 21, 21, 21, 27, 30, 24, 18, 27, 24, 27, 24, 33, 21, 24, 24, 18, 24, 33, 21, 24, 33, 33, 24, 18, 30, 24, 15, 30, 27, 24, 24, 18, 21, 21, 24, 30, 24, 24, 27, 33, 18, 24, 30, 24, 27, 24, 30, 21, 27, 27, 24, 30, 33, 24, 18, 24, 27, 21, 24, 27, 30, 24, 24, 24, 27, 18, 24, 24, 24, 24, 24, 24, 27, 24, 33, 30, 24, 24, 27, 24, 27, 24, 27, 18, 18, 24, 24, 27, 24, 24, 18, 36, 24, 24, 18, 24, 21, 24, 30, 24, 27, 30, 33, 30, 27, 30, 27, 24, 27, 24, 30, 21, 30, 27, 30, 30, 24, 18, 30, 18, 24, 21, 30, 24, 27, 24, 30, 27, 30, 24, 18, 27, 24, 24, 30, 30, 27, 27, 24, 24, 24, 30, 24, 24, 21, 24, 27, 27, 24, 27, 24, 30, 27, 24, 30, 24, 27, 21, 24, 24, 24, 30, 24, 27, 30, 24, 24, 21, 21, 30, 27, 24, 27, 30, 33, 21, 30, 24, 24, 24, 24, 21, 27, 24, 24, 33, 24, 24, 24, 30, 27, 24, 30, 27, 24, 18, 24, 27, 18, 33, 21, 27, 24, 24, 24, 24, 24, 30, 27, 36, 24, 24, 15, 27, 24, 24, 27, 18, 30, 18, 27, 24, 30, 24, 6, 27, 30, 30, 24, 24, 15, 30, 24, 21, 24, 24, 24, 30, 24, 36, 18, 21, 30, 27, 30, 27, 24, 24, 30, 24, 27, 36, 24, 24, 24, 30, 30, 24, 21, 24, 36, 18, 27, 30, 24, 27, 36, 33, 24, 33, 27, 24, 30, 24, 27, 27, 24, 21, 33, 27, 30, 24, 24, 27, 30, 12, 30, 30, 24, 27, 24, 24, 30, 24, 30, 24, 24, 18, 33, 27, 30, 30, 24, 24, 30, 30, 27, 30, 33, 18, 30, 33, 24, 24, 24, 18, 24, 21, 21, 21, 30, 30, 24, 24, 36, 24, 24, 27, 21, 33, 33, 18, 24, 36, 27, 27, 24, 24, 24, 30, 27, 24, 30, 21, 24, 36, 30, 24, 27, 27, 24, 27, 21, 24, 27, 24, 33, 21, 21, 33, 27, 24, 33, 27, 24, 18, 6, 24, 24, 21, 24, 30, 30, 24, 24, 24, 24, 27, 33, 24, 24, 27, 24, 18, 15, 27, 27, 12, 21, 30, 27, 36, 36, 24, 24, 27, 18, 27, 24, 30, 30, 24, 24, 24, 27, 21, 24, 30, 24, 24, 30, 30, 24, 27, 33, 24, 27, 24, 30, 24, 30, 30, 24, 27, 24, 30, 24, 24, 27, 24, 33, 24, 18, 18, 24, 27, 21, 27, 27, 33, 30, 30, 33, 21, 27, 27, 24, 24, 18, 30, 24, 24, 24, 24, 30, 30, 27, 21, 27, 27, 27, 30, 21, 24, 24, 24, 24, 24, 30, 24, 36, 24, 24, 21, 27, 21, 27, 30, 24, 33, 24, 24, 18, 30, 30, 24, 24, 24, 12, 30, 27, 24, 27, 24, 24, 18, 30, 27, 27, 12, 27, 24, 24, 33, 24, 30, 30, 27, 24, 21, 24, 27, 24, 6, 30, 18, 27, 24, 30, 24, 24, 24, 18, 24, 36, 18, 33, 24, 24, 24, 30, 27, 18, 24, 24, 33, 27, 24, 27, 27, 30, 24, 30, 24, 27, 24, 24, 30, 18, 27, 36, 24, 24, 24, 30, 27, 24, 27, 30, 24, 30, 36, 36, 21, 24, 30, 27, 24, 18, 27, 24, 24, 30, 21, 24, 30, 24, 27, 30, 24, 27, 27, 24, 24, 30, 30, 33, 24, 24, 21, 24, 27, 36, 24, 24, 27, 24, 33, 24, 21, 24, 24, 27, 27, 24, 30, 24, 27, 12, 27, 24, 30, 30, 18, 27, 27, 24, 24, 24, 24, 24, 30, 18, 30, 27, 24, 30, 33, 30, 33, 30, 27, 24, 27, 24, 30, 21, 24, 24, 30, 36, 24, 33, 12, 24, 33, 24, 27, 36, 30, 24, 27, 24, 27, 27, 30, 24, 24, 24, 33, 30, 24, 27, 30, 18, 21, 24, 21, 24, 27, 18, 24, 30, 30, 24, 24, 24, 30, 30, 21, 24, 24, 24, 21, 27, 12, 27, 24, 21, 33, 21, 24, 30, 21, 33, 33, 24, 27, 27, 24, 21, 30, 27, 24, 30, 27, 27, 24, 24, 27, 33, 27, 27, 24, 24, 30, 24, 33, 24, 21, 24, 27, 27, 24, 24, 21, 30, 24, 27, 24, 24, 30, 24, 24, 24, 24, 24, 24, 21, 33, 30, 27, 24, 30, 24, 24, 24, 24, 33, 33, 24, 30, 27, 24, 27, 24, 24, 18, 21, 27, 27, 36, 24, 24, 30, 30, 24, 24, 27, 30, 36, 24, 30, 27, 30, 27, 27, 21, 24, 24, 24, 24, 15, 33, 18, 27, 27, 21, 27, 24, 27, 30, 27, 33, 30, 27, 27, 30, 24, 27, 21, 33, 30, 27, 24, 36, 24, 30, 24, 30, 24, 18, 24, 24, 30, 24, 30, 33, 18, 21, 24, 18, 24, 24, 27, 30, 24, 24, 27, 30, 30, 24, 18, 36, 30, 24, 30, 36, 27, 27, 30, 27, 36, 27, 33, 24, 30, 21, 27, 24, 30, 18, 21, 24, 30, 24, 24, 30, 27, 24, 30, 24, 24, 18, 12, 24, 27, 18, 27, 21, 27, 21, 21, 18, 27, 27, 24, 24, 24, 27, 18, 33, 24, 33, 30, 24, 18, 27, 24, 30, 24, 24, 30, 27, 24, 27, 24, 27, 24, 24, 24, 21, 24, 15, 24, 30, 24, 24, 30, 24, 27, 24, 30, 24, 18, 24, 30, 30, 18, 24, 24, 24, 18, 18, 27, 18, 12, 24, 27, 27, 24, 24, 30, 24, 30, 24, 30, 21, 24, 18, 24, 27, 18, 21, 24, 6, 24, 24, 24, 21, 27, 27, 12, 18, 24, 24, 24, 33, 21, 15, 33, 21, 24, 30, 24, 27, 24, 24, 27, 24, 24, 33, 30, 24, 27, 27, 21, 30, 30, 30, 33, 27, 6, 30, 24, 24, 27, 33, 30, 27, 24, 24, 30, 24, 24, 24, 27, 27, 24, 27, 33, 24, 24, 27, 27, 27, 27, 24, 24, 27, 21, 30, 30, 27, 24, 24, 24, 24, 30, 30, 30, 24, 24, 24, 24, 30, 30, 24, 24, 30, 24, 24, 24, 33, 24, 33, 24, 18, 33, 27, 24, 27, 24, 24, 27, 24, 24, 24, 27, 27, 24, 30, 24, 27, 24, 27, 27, 30, 27, 30, 27, 33, 18, 15, 24, 33, 24, 30, 24, 24, 24, 18, 24, 21, 24, 24, 15, 30, 24, 21, 27, 33, 27, 30, 30, 27, 27, 24, 24, 24, 27, 24, 24, 33, 24, 36, 18, 21, 21, 24, 27, 18, 27, 24, 18, 30, 21, 24, 27, 24, 33, 27, 27, 27, 30, 27, 27, 30, 27, 24, 30, 27, 24, 24, 27, 21, 33, 24, 21, 30, 24, 24, 18, 27, 33, 30, 30, 24, 27, 18, 24, 24, 27, 24, 30, 24, 30, 24, 30, 30, 36, 24, 24, 36, 24, 15, 24, 27, 24, 27, 30, 24, 30, 24, 30, 27, 27, 33, 15, 30, 27, 30, 24, 24, 21, 24, 6, 21] [26, 27, 25, 29, 28, 24, 30, 29, 28, 29, 26, 27, 24, 25, 23, 29, 27, 27, 21, 23, 27, 21, 30, 31, 25, 27, 22, 27, 27, 25, 29, 29, 23, 27, 28, 30, 29, 26, 28, 24, 21, 25, 23, 26, 26, 26, 26, 30, 26, 22, 30, 24, 28, 28, 25, 28, 25, 24, 28, 22, 26, 25, 25, 24, 28, 27, 24, 27, 25, 28, 28, 30, 27, 26, 26, 26, 30, 28, 27, 27, 27, 25, 24, 24, 30, 27, 28, 28, 24, 24, 24, 32, 30, 26, 28, 24, 25, 28, 26, 24, 29, 26, 28, 28, 30, 27, 31, 27, 26, 25, 30, 27, 26, 28, 30, 21, 26, 28, 30, 27, 26, 30, 27, 25, 23, 25, 27, 27, 28, 30, 28, 28, 29, 27, 25, 30, 25, 27, 24, 26, 31, 27, 26, 28, 27, 23, 31, 26, 28, 26, 25, 23, 30, 28, 28, 26, 28, 26, 29, 26, 28, 30, 24, 25, 27, 30, 26, 28, 23, 26, 26, 25, 25, 26, 26, 27, 26, 25, 25, 28, 28, 25, 27, 27, 28, 26, 22, 27, 23, 26, 25, 29, 27, 30, 22, 27, 29, 28, 34, 26, 27, 27, 29, 27, 31, 27, 25, 23, 23, 27, 24, 25, 26, 25, 29, 27, 30, 26, 27, 28, 24, 28, 28, 26, 30, 24, 27, 25, 25, 28, 25, 30, 23, 27, 25, 24, 26, 26, 28, 25, 25, 28, 31, 26, 23, 32, 28, 29, 26, 26, 30, 27, 24, 31, 26, 25, 28, 30, 28, 25, 28, 25, 29, 27, 27, 28, 28, 25, 27, 27, 26, 30, 25, 27, 27, 25, 28, 26, 28, 27, 24, 26, 24, 26, 30, 22, 30, 26, 27, 30, 28, 29, 25, 28, 26, 29, 28, 27, 26, 29, 26, 30, 28, 29, 29, 28, 27, 23, 27, 27, 26, 24, 27, 28, 30, 29, 27, 27, 26, 29, 26, 27, 26, 28, 32, 27, 26, 26, 31, 29, 25, 28, 32, 26, 30, 25, 24, 29, 27, 25, 28, 27, 28, 33, 25, 26, 25, 27, 26, 24, 23, 24, 24, 25, 25, 24, 21, 23, 30, 29, 25, 31, 28, 27, 26, 26, 25, 31, 27, 29, 25, 27, 23, 28, 29, 29, 28, 26, 29, 25, 29, 25, 31, 28, 26, 24, 26, 28, 30, 27, 26, 23, 23, 27, 30, 28, 28, 26, 23, 25, 29, 27, 33, 28, 27, 26, 26, 26, 29, 28, 28, 26, 26, 23, 25, 31, 23, 25, 23, 26, 29, 29, 29, 28, 30, 26, 27, 30, 29, 26, 28, 26, 31, 27, 26, 26, 28, 26, 28, 27, 25, 26, 24, 28, 30, 28, 21, 25, 27, 23, 28, 29, 22, 26, 29, 25, 25, 29, 27, 26, 28, 25, 26, 26, 26, 24, 27, 26, 25, 29, 25, 23, 29, 26, 24, 27, 29, 26, 28, 27, 27, 24, 29, 30, 26, 26, 29, 30, 26, 26, 25, 27, 26, 26, 33, 30, 28, 28, 26, 23, 25, 30, 28, 25, 32, 26, 27, 25, 25, 25, 27, 24, 26, 28, 27, 26, 29, 25, 24, 28, 29, 27, 27, 25, 25, 24, 24, 27, 28, 27, 28, 34, 26, 26, 26, 26, 27, 26, 28, 30, 26, 26, 21, 23, 27, 27, 26, 31, 27, 24, 27, 27, 29, 30, 30, 29, 29, 27, 26, 29, 24, 28, 29, 25, 25, 28, 26, 29, 27, 31, 30, 25, 27, 27, 23, 25, 27, 30, 27, 27, 29, 30, 31, 26, 28, 30, 22, 23, 27, 25, 27, 24, 25, 23, 29, 26, 27, 24, 24, 24, 33, 26, 24, 29, 28, 27, 25, 25, 27, 28, 26, 25, 28, 26, 28, 28, 30, 24, 24, 25, 24, 25, 28, 25, 25, 25, 28, 28, 24, 28, 27, 23, 28, 26, 26, 25, 24, 26, 24, 28, 29, 26, 27, 26, 25, 23, 25, 28, 26, 24, 26, 28, 24, 28, 27, 26, 27, 29, 25, 24, 27, 28, 24, 27, 27, 27, 29, 27, 25, 28, 23, 24, 28, 24, 25, 30, 26, 29, 26, 29, 30, 26, 26, 27, 26, 25, 28, 27, 23, 24, 26, 26, 27, 25, 26, 24, 35, 25, 26, 23, 27, 24, 28, 28, 26, 27, 29, 28, 29, 27, 26, 27, 23, 26, 25, 29, 25, 30, 27, 31, 30, 24, 23, 26, 23, 26, 24, 29, 29, 26, 23, 28, 29, 27, 27, 25, 26, 29, 25, 30, 28, 26, 28, 26, 24, 28, 29, 27, 26, 24, 25, 27, 30, 26, 27, 26, 27, 27, 25, 28, 24, 30, 25, 25, 26, 28, 30, 26, 28, 29, 28, 27, 24, 24, 30, 28, 26, 28, 30, 29, 25, 30, 28, 25, 27, 27, 24, 28, 26, 26, 29, 27, 27, 26, 30, 32, 25, 28, 28, 26, 25, 24, 28, 24, 28, 24, 27, 26, 26, 25, 26, 27, 27, 26, 31, 24, 25, 23, 27, 26, 26, 29, 24, 26, 22, 31, 27, 27, 26, 21, 30, 29, 30, 25, 29, 22, 29, 26, 24, 27, 26, 25, 27, 26, 32, 24, 24, 28, 29, 28, 29, 28, 25, 28, 23, 28, 29, 26, 25, 27, 31, 28, 26, 24, 26, 29, 22, 27, 28, 25, 26, 29, 31, 26, 29, 26, 27, 27, 25, 27, 28, 26, 28, 29, 26, 29, 26, 27, 29, 29, 23, 25, 30, 24, 30, 27, 27, 27, 26, 29, 26, 26, 23, 29, 27, 29, 28, 28, 24, 30, 30, 27, 29, 30, 24, 32, 27, 28, 25, 26, 24, 29, 25, 24, 25, 26, 28, 25, 25, 31, 24, 27, 29, 24, 29, 29, 23, 27, 28, 28, 27, 25, 25, 28, 28, 28, 26, 28, 29, 28, 28, 29, 26, 27, 27, 25, 27, 24, 27, 28, 25, 30, 26, 24, 28, 26, 25, 31, 26, 29, 24, 21, 25, 26, 25, 25, 29, 29, 25, 26, 31, 26, 28, 31, 26, 26, 27, 26, 23, 22, 25, 27, 22, 26, 29, 28, 31, 34, 26, 26, 27, 23, 25, 28, 27, 29, 28, 25, 28, 26, 26, 27, 29, 25, 27, 29, 27, 25, 28, 31, 25, 28, 26, 28, 27, 30, 29, 24, 28, 28, 28, 28, 24, 27, 24, 30, 27, 23, 26, 26, 30, 25, 26, 26, 28, 30, 30, 30, 26, 28, 27, 25, 27, 24, 29, 26, 24, 24, 27, 26, 28, 28, 24, 28, 27, 29, 27, 25, 25, 26, 27, 27, 26, 29, 27, 32, 25, 26, 24, 26, 25, 26, 26, 28, 30, 27, 27, 24, 30, 28, 24, 25, 25, 22, 27, 27, 26, 25, 24, 25, 23, 32, 27, 28, 22, 28, 25, 25, 30, 25, 30, 28, 26, 25, 26, 27, 26, 27, 21, 28, 26, 27, 27, 27, 26, 27, 26, 24, 26, 30, 24, 32, 26, 27, 25, 28, 27, 24, 27, 27, 29, 27, 27, 28, 27, 28, 25, 29, 27, 28, 24, 27, 31, 24, 30, 28, 28, 26, 26, 30, 26, 26, 27, 28, 27, 28, 32, 36, 25, 26, 26, 30, 28, 24, 28, 27, 27, 26, 25, 25, 26, 26, 28, 26, 28, 25, 29, 28, 26, 29, 28, 27, 25, 29, 24, 25, 29, 28, 27, 27, 29, 26, 28, 28, 25, 26, 28, 27, 28, 27, 28, 28, 26, 22, 29, 26, 28, 29, 22, 28, 28, 27, 28, 23, 25, 28, 29, 23, 27, 29, 28, 26, 29, 26, 30, 28, 26, 25, 26, 30, 27, 25, 24, 27, 32, 34, 28, 28, 21, 27, 31, 28, 29, 29, 27, 25, 27, 28, 26, 27, 27, 25, 26, 26, 28, 25, 26, 28, 28, 23, 27, 26, 25, 25, 26, 24, 25, 29, 26, 27, 25, 23, 26, 29, 25, 26, 26, 26, 25, 30, 22, 28, 25, 25, 29, 26, 25, 27, 26, 29, 28, 27, 26, 27, 29, 25, 27, 28, 25, 30, 30, 26, 27, 27, 28, 31, 31, 26, 25, 27, 30, 28, 30, 28, 25, 28, 31, 27, 26, 24, 24, 31, 23, 29, 26, 26, 30, 26, 26, 27, 27, 25, 24, 24, 28, 29, 27, 26, 27, 25, 27, 25, 27, 29, 29, 27, 29, 27, 27, 28, 25, 27, 22, 25, 27, 27, 29, 26, 26, 27, 29, 25, 25, 30, 27, 29, 25, 28, 28, 27, 29, 26, 25, 26, 29, 26, 26, 27, 32, 24, 27, 25, 24, 28, 29, 27, 29, 25, 26, 29, 27, 27, 26, 26, 26, 24, 30, 27, 27, 25, 31, 23, 28, 27, 28, 27, 24, 30, 27, 28, 25, 27, 29, 24, 24, 25, 23, 27, 29, 28, 28, 27, 25, 26, 28, 30, 25, 24, 31, 26, 25, 27, 30, 26, 27, 27, 28, 30, 28, 29, 26, 32, 25, 26, 26, 28, 24, 22, 24, 29, 26, 24, 28, 26, 29, 27, 26, 25, 24, 22, 26, 26, 24, 27, 25, 28, 24, 24, 23, 27, 26, 26, 26, 27, 28, 23, 31, 23, 28, 28, 26, 23, 27, 28, 28, 27, 26, 30, 27, 26, 27, 25, 24, 24, 29, 26, 25, 25, 22, 27, 29, 25, 26, 28, 26, 26, 25, 29, 25, 23, 25, 28, 25, 23, 26, 26, 26, 25, 23, 28, 24, 23, 26, 27, 26, 26, 26, 28, 24, 29, 24, 28, 24, 25, 23, 27, 31, 25, 27, 27, 21, 26, 27, 27, 23, 29, 28, 22, 23, 25, 28, 26, 27, 24, 23, 30, 26, 27, 30, 30, 28, 25, 24, 25, 25, 25, 31, 28, 26, 26, 30, 24, 28, 29, 28, 29, 30, 21, 28, 26, 27, 27, 31, 30, 27, 26, 26, 29, 26, 25, 24, 27, 28, 26, 27, 33, 28, 24, 26, 27, 27, 28, 26, 24, 28, 25, 27, 33, 28, 27, 26, 28, 25, 29, 29, 29, 26, 29, 24, 26, 25, 29, 25, 26, 28, 25, 25, 26, 29, 26, 29, 26, 23, 28, 26, 27, 27, 24, 25, 29, 24, 25, 25, 27, 28, 27, 28, 27, 27, 27, 28, 29, 27, 28, 31, 27, 31, 24, 22, 28, 31, 25, 29, 26, 26, 27, 23, 26, 24, 26, 27, 22, 32, 28, 24, 27, 29, 27, 29, 29, 28, 27, 26, 28, 26, 28, 25, 24, 29, 27, 28, 24, 25, 24, 27, 28, 24, 28, 27, 25, 29, 25, 28, 26, 28, 28, 28, 27, 28, 28, 28, 28, 28, 29, 26, 27, 26, 26, 24, 29, 24, 34, 25, 26, 29, 27, 26, 24, 28, 29, 28, 27, 27, 26, 22, 26, 26, 29, 24, 34, 27, 28, 25, 29, 26, 31, 27, 26, 29, 27, 20, 27, 28, 29, 25, 27, 27, 29, 26, 28, 26, 27, 32, 22, 28, 28, 29, 26, 29, 24, 25, 21, 24]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15455695078749254\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "print(cohen_kappa_score(y_val,y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
